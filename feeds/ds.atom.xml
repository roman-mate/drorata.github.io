<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Dr. Dror - DS</title><link href="http://drorata.github.io/" rel="alternate"></link><link href="http://drorata.github.io/feeds/ds.atom.xml" rel="self"></link><id>http://drorata.github.io/</id><updated>2018-05-08T12:41:16+02:00</updated><entry><title>Expose an API for a trained prediction model</title><link href="http://drorata.github.io/posts/2018/May/08/expose-an-api-for-a-trained-prediction-model/" rel="alternate"></link><published>2018-05-08T12:41:16+02:00</published><updated>2018-05-08T12:41:16+02:00</updated><author><name>Dror Atariah</name></author><id>tag:drorata.github.io,2018-05-08:/posts/2018/May/08/expose-an-api-for-a-trained-prediction-model/</id><summary type="html">&lt;p&gt;You will build a RESTful API exposing a trained prediction model.&lt;/p&gt;</summary><content type="html">&lt;h2 id="why"&gt;Why?&lt;/h2&gt;
&lt;p&gt;This is rather clear.
As a data scientist, you worked hard.
You got the data, you explored it, and identified the important features.
Next, you spent days wondering in the mighty hyper-parameters space.
Bottom line, you have trained the best model which can now become part of the production environment.
Alas, to that end you need to turn your model into something more interactive.
In this post, you will learn how to get started with &lt;a href="http://flask.pocoo.org/"&gt;Flask&lt;/a&gt; and expose a RESTful API that can communicate with the trained model.&lt;/p&gt;
&lt;h2 id="training-the-model"&gt;Training the model&lt;/h2&gt;
&lt;p&gt;This is out of the scope of this post.
In what follows I assume you have something like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.externals&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;joblib&lt;/span&gt;
&lt;span class="n"&gt;pipeline&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;joblib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pipeline&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;pipeline.pkl&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Where &lt;code&gt;pipeline&lt;/code&gt; is a Scikit-learn pipeline.
Pay attention to the nature of the unseen data points that you would like to feed into the pipeline using the API.
You should make sure that the pipeline can handle the unseen data points; for example take care of preprocessing steps.
If this is not the case, you have to take care and enable the application to preprocess the data.&lt;/p&gt;
&lt;h2 id="flask-application"&gt;Flask application&lt;/h2&gt;
&lt;p&gt;Include the following snippet in a file &lt;code&gt;app.py&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# app.py&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;flask&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;jsonify&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.externals&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;joblib&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;train_model&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;tm&lt;/span&gt; &lt;span class="c1"&gt;# Implementation of preprocessing steps outside of&lt;/span&gt;
                         &lt;span class="c1"&gt;# the pipeline&amp;#39;s scope&lt;/span&gt;
&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flask&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="nd"&gt;@app.route&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/predict&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;methods&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;POST&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;json_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_json&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;query_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prepare_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query_df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prediction&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;jsonify&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;prediction&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;prediction&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;clf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;joblib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;pipeline.pkl&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;debug&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;0.0.0.0&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is it.
Execute &lt;code&gt;python app.py&lt;/code&gt; and send unseen data points to &lt;code&gt;0.0.0.0:5000&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="asking-the-application-for-predictions"&gt;Asking the application for predictions&lt;/h2&gt;
&lt;p&gt;Assume you have &lt;code&gt;data.json&lt;/code&gt; having the following JSON with unseen data points:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;feat1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;0&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;feat2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;0&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;3.14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;2.71&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can send this JSON to your already running using &lt;a href="https://curl.haxx.se/"&gt;&lt;code&gt;curl&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl -H &lt;span class="s2"&gt;&amp;quot;Content-Type: application/json&amp;quot;&lt;/span&gt; --data @data.json http://0.0.0.0:5000/predict
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or using &lt;a href="https://httpie.org/"&gt;&lt;code&gt;httpie&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;http POST http://0.0.0.0:5000/predict &amp;lt; data.json
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It is as simple as that.
Of course, you are likely to need way more, but I am sure this will serve as a good start.&lt;/p&gt;
&lt;h2 id="minimal-working-example"&gt;Minimal working example&lt;/h2&gt;
&lt;p&gt;Simplest way to experience a prediction model API is to run the docker image I prepared:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker run -p &lt;span class="m"&gt;5000&lt;/span&gt;:5000 drorata/pm_api:v1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once the container is running you can throw at it the following JSON:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Age&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;53&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;111&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Cabin&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;53&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;C23 C25 C27&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;111&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Embarked&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;53&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;S&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;111&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Q&amp;quot;&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Fare&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;53&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;263&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;111&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;7.7792&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;53&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Fortune, Miss. Ethel Flora&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;111&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Shine, Miss. Ellen Natalia&amp;quot;&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Parch&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;53&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;111&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;PassengerId&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;53&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;945&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;111&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1003&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Pclass&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;53&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;111&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Sex&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;53&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;female&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;111&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;female&amp;quot;&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;SibSp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;53&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;111&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Ticket&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;53&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;19950&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;111&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;330968&amp;quot;&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you want to see more details, this image is built using the sources available in &lt;a href="https://github.com/drorata/pm_api"&gt;this repository&lt;/a&gt;.
You can create a local python environment, install the requirements and enjoy the example.&lt;/p&gt;</content><category term="python"></category><category term="docker"></category><category term="model"></category><category term="sklearn"></category><category term="IT"></category></entry><entry><title>Messages taken home from "AI in Data Science presented by RecdoTech"</title><link href="http://drorata.github.io/posts/2017/Sep/22/messages-taken-home-from-ai-in-data-science-presented-by-recdotech/" rel="alternate"></link><published>2017-09-22T21:55:48+02:00</published><updated>2017-09-22T22:00:05+02:00</updated><author><name>Dror Atariah</name></author><id>tag:drorata.github.io,2017-09-22:/posts/2017/Sep/22/messages-taken-home-from-ai-in-data-science-presented-by-recdotech/</id><summary type="html">&lt;p&gt;Some remarks and highlights from taken from a meetup I attended.&lt;/p&gt;</summary><content type="html">&lt;p&gt;On September 14th (2017) I attended the first meet organized by RecdoTech&lt;sup id="fnref-b5d84a8f"&gt;&lt;a class="footnote-ref" href="#fn-b5d84a8f"&gt;2&lt;/a&gt;&lt;/sup&gt;.
Although organized by be a recruitment agency, it turned out to be a real meetup.
There should have been three talks, but eventually there were only two.&lt;/p&gt;
&lt;p&gt;The first talk, by &lt;a href="https://www.linkedin.com/in/luba-weissmann-47b7757/"&gt;Luba Weissmann&lt;/a&gt;, introduce the basics concepts and ideas from the world of credit scoring.
In particular, Luba discussed how FinTech harnesses data science to understand behavioral aspects of individuals asking for credit.
The speaker tried to make a very clear distinction between &lt;em&gt;machine learning&lt;/em&gt; and &lt;em&gt;AI&lt;/em&gt;; for her the main differentiator is the way the decision is made.
Personally, I was not convinced, and I think primary reason to use the term AI in this context, is to create more hype.
It is often the case, that AI is considered an umbrella of different automated decision making approaches.
Many of them are employing machine learning, but not only.&lt;sup id="fnref-5fa6e1e7"&gt;&lt;a class="footnote-ref" href="#fn-5fa6e1e7"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;An interesting point raised by Luba is that regulators are introducing hurdles to the integration of AI (in the broader sense) into credit scoring.
The reason might be a little surprising: in a nutshell, when credit is denied, the customer (let it be an individual or a company) has the right to know why the rejection happened.
However, assuming the reason for rejection is due to an algorithm which returned some value, it is not necessarily clear and trivial to explain why and in turn it may be hard to explain this to the customer.
Have deep learning in mind, as a clear example where the algorithm may suggest rejection, but understanding &lt;em&gt;why&lt;/em&gt; is very complicated.&lt;/p&gt;
&lt;p&gt;The second talk, by &lt;a href="https://www.linkedin.com/in/sfoucaud/"&gt;Sébastien Foucaud&lt;/a&gt;, was more high level, and provided ideas and insights into the meaning of having data science as part of business landscape.
In the business world, the results of data science work have to have impact.
The needle has to move!&lt;/p&gt;
&lt;p&gt;&lt;img alt="VU meter" src="http://drorata.github.io/images/VU_Meter.gif"&gt;&lt;/p&gt;
&lt;p&gt;The data scientist who wishes to survive and strive in the business world, has to add value to the operations.
Even research labs ran by big players are, at the end of the day, about adding value to the company.
This holds for sure when it comes to small/medium size companies and startups, where the impact has to be present.
Sébastien used the term &lt;em&gt;business driven data scientist&lt;/em&gt;; I liked it especially when there is so much noise around the data drivenness approach.
It is impossible to be data driven without, first, be business driven.
Along this line of thought, Sébastien also mentioned that the model does not matter; it is its integration and impact that count.
To that end, if you have a model which delivers then no one cares about its beauty, complexity or how ingenious it is.
As a scientist this is sometimes hard to accept; you are trained to deliver perfection, because otherwise your paper will not be accepted.&lt;/p&gt;
&lt;p&gt;Another point discussed during the talk was related to data quality.
"Not all data is useful" and you need your data to be "smart".
Having big data does not necessarily imply it is smart.
Sometimes, the overhead derived by big data is so much bigger, that at the end of the day the impact is marginal.
The most important key to &lt;em&gt;smart data&lt;/em&gt; is to have it labeled or flagged.
This is the only way that the data scientist can really derive insights out of it.
Facebook, Google, Airbnb and all the others are releasing technologies like crazy.
They are much more cautious when it comes to sharing labeled data&lt;sup id="fnref-9f4e79c9"&gt;&lt;a class="footnote-ref" href="#fn-9f4e79c9"&gt;3&lt;/a&gt;&lt;/sup&gt;.
Companies are trying to improve their data quality all the time and user feedback is a central ingredient.
For instance, think of all the thumbs-up/down icons that turned into a little epidemic.
They are all there as part of the effort of companies to label, tag and flag their data.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Thumbs up" src="http://drorata.github.io/images/thumbs-up.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Lastly, the composition of a data science team was briefly discussed during the Q&amp;amp;A part.
In contrast to the vibe around the &lt;a href="http://lmgtfy.com/?q=full+stack+data+scientist"&gt;full stack data scientist&lt;/a&gt;, Sébastien advocated the need of a cross functional team in order to address the needs.
The team should consist of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data scientist and engineer (assuming these are two different things): I guess these two are kind of self-explanatory.&lt;/li&gt;
&lt;li&gt;Product manager: this guy should fill in the gap between the data scientist in the team and the "business driven" data scientist that should be there. But not only for that; he should take care of all the coordination and the project management.&lt;/li&gt;
&lt;li&gt;Software developer/engineer: enable easier streamlining of the data science products into the business and to production environments.&lt;/li&gt;
&lt;li&gt;User interface: remember the thumbs-up/down, this guys is needed to make user feedback more accessible.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All in all, the teams size should be between 5-8 people, depending on the plate's size, the capacity of each team member and its availability.
I believe that during an initial phase of integration of the team(s) in the organization,  some of the manpower may be shared with other parts of the tech team.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn-5fa6e1e7"&gt;
&lt;p&gt;Nice summary on &lt;a href="https://www.quora.com/What-are-the-main-differences-between-artificial-intelligence-and-machine-learning-Is-machine-learning-a-part-of-artificial-intelligence"&gt;Quora&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-5fa6e1e7" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-b5d84a8f"&gt;
&lt;p&gt;This is a brand of Darwin Recruitment. See &lt;a href="https://www.meetup.com/RecdoTech/members/230799589/"&gt;profile&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-b5d84a8f" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-9f4e79c9"&gt;
&lt;p&gt;There are &lt;a href="https://quickdraw.withgoogle.com/data"&gt;exceptions&lt;/a&gt; of course.&amp;#160;&lt;a class="footnote-backref" href="#fnref-9f4e79c9" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content></entry><entry><title>Benchmarking Columns Operations</title><link href="http://drorata.github.io/posts/2017/Jun/27/benchmarking-columns-operations/" rel="alternate"></link><published>2017-06-27T21:02:26+02:00</published><updated>2017-06-27T21:02:26+02:00</updated><author><name>Dror Atariah</name></author><id>tag:drorata.github.io,2017-06-27:/posts/2017/Jun/27/benchmarking-columns-operations/</id><summary type="html">&lt;p&gt;Benchmarking different ways to process two columns simultaneously.&lt;/p&gt;</summary><content type="html">&lt;p&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;I recently ran the following experiment.
The reason was my need to perform operations on two columns.
Think for example in terms of features engineering; you want to produce an new feature (i.e. column) which is the ratio of the some other two columns.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[1]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[2]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;foo&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;bar&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[3]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt output_prompt"&gt;Out[3]:&lt;/div&gt;



&lt;div class="output_html rendered_html output_subarea output_execute_result"&gt;
&lt;div&gt;
&lt;style&gt;
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
&lt;/style&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;foo&lt;/th&gt;
      &lt;th&gt;bar&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.065648&lt;/td&gt;
      &lt;td&gt;0.593402&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.592051&lt;/td&gt;
      &lt;td&gt;0.123502&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0.621030&lt;/td&gt;
      &lt;td&gt;0.629470&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0.210630&lt;/td&gt;
      &lt;td&gt;0.462535&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.263708&lt;/td&gt;
      &lt;td&gt;0.807304&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Iterating-over-the-rows"&gt;Iterating over the rows&lt;a class="anchor-link" href="#Iterating-over-the-rows"&gt;&amp;#182;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Here we iterate over the rows, and access the indexes of the &lt;code&gt;pd.Series&lt;/code&gt; which represents the row.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[4]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="k"&gt;timeit&lt;/span&gt;
pd.Series([
    x[1][&amp;#39;foo&amp;#39;] * x[1][&amp;#39;bar&amp;#39;] for x in df.iterrows()
])
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;518 ms ± 60 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Applying-a-function"&gt;Applying a function&lt;a class="anchor-link" href="#Applying-a-function"&gt;&amp;#182;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Here we use &lt;code&gt;pd.DataFrame.apply&lt;/code&gt; and provide the axis.
This is actually, the solution I had to use in a case which I now longer remember its details.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[5]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="k"&gt;timeit&lt;/span&gt;
df.apply(lambda x: x[&amp;#39;foo&amp;#39;] * x[&amp;#39;bar&amp;#39;], axis=1)
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;299 ms ± 43.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Columns-operations"&gt;Columns operations&lt;a class="anchor-link" href="#Columns-operations"&gt;&amp;#182;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Best and fastest approach; at least for this simple case.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="cell border-box-sizing code_cell rendered"&gt;
&lt;div class="input"&gt;
&lt;div class="prompt input_prompt"&gt;In&amp;nbsp;[6]:&lt;/div&gt;
&lt;div class="inner_cell"&gt;
    &lt;div class="input_area"&gt;
&lt;div class=" highlight hl-ipython3"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;%%&lt;/span&gt;&lt;span class="k"&gt;timeit&lt;/span&gt;
df.foo.mul(df.bar)
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="output_wrapper"&gt;
&lt;div class="output"&gt;


&lt;div class="output_area"&gt;

&lt;div class="prompt"&gt;&lt;/div&gt;


&lt;div class="output_subarea output_stream output_stdout output_text"&gt;
&lt;pre&gt;97.2 µs ± 11.9 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="Summary"&gt;Summary&lt;a class="anchor-link" href="#Summary"&gt;&amp;#182;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Obviously, for the simple multiplication, the last option is the most pythonic (and the fastest as well).
But, still due to an edge case I decided to run this test.
Maybe someone would find it helpful.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
 

&lt;/p&gt;</content><category term="python"></category><category term="pandas"></category></entry><entry><title>Remarks on "Data Driven" by Patil &amp; Mason</title><link href="http://drorata.github.io/posts/2017/Mar/20/remarks-on-data-driven-by-patil-mason/" rel="alternate"></link><published>2017-03-20T09:08:14+01:00</published><updated>2017-03-20T09:08:14+01:00</updated><author><name>Dror Atariah</name></author><id>tag:drorata.github.io,2017-03-20:/posts/2017/Mar/20/remarks-on-data-driven-by-patil-mason/</id><summary type="html">&lt;p&gt;Some remarks, comments and ideas discussed after reading the paper "Data Driven - creating a data culture"&lt;/p&gt;</summary><content type="html">&lt;p&gt;I read recently the paper &lt;em&gt;"Data Driven - creating a data culture"&lt;/em&gt; by DJ Patil and Hilary Mason (&lt;a href="http://www.oreilly.com/data/free/data-driven.csp"&gt;link&lt;/a&gt;).
In this post, I'd like to share some thoughts/remarks/etc. that I collected while reading.&lt;/p&gt;
&lt;h2 id="communication-skills"&gt;Communication skills&lt;/h2&gt;
&lt;p&gt;Rather early on the authors mention the importance of &lt;a href="http://drorata.github.io/posts/2017/Feb/17/thoughts-about-data-science-teams/"&gt;communication skills&lt;/a&gt;.
For "Asking the right questions" one need to be able to communicate with different stakeholders.
As a data science team, you need to understand what are the pain points of your colleagues and to that end you would have to speak with them.
You could have Spock, Lieutenant Data, Pinky and the Brain on board but if they can only talk to each other things won't move forward.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Date heroes" src="http://drorata.github.io/images/data-characters.jpg"&gt;&lt;/p&gt;
&lt;h2 id="data-democratization"&gt;Data democratization&lt;/h2&gt;
&lt;p&gt;I totally agree that making sure the data of the organization is accessible is important.
But, there's a bit of a chicken and egg problem here.
Stakeholders won't look into your amazing data warehouse if it is not clean, understandable, accessible etc.
But in order to have your data ready for everyone a lot of effort has to be invested.
Having a sound foundation inside the data science team  can be the solution.
Make sure that the data exploration processes are as simple and straightforward as possible for the DS team to start with.
This will result in a nice toy that you can show off around and attract others to dirty their hands with the data.
In turn, it will be needed to extend the available data, the circles of users etc.&lt;/p&gt;
&lt;h2 id="ask-the-right-questions"&gt;Ask the right questions&lt;/h2&gt;
&lt;p&gt;First, make sure you don't answer what you can but what you should.
This means you have to invest a lot of time in understanding the problem.
"Starting with the data" as suggested by the scientific method is dangerous as it may mask the business need.
The formulation of the question has to be derived from the business needs and not from the data.
Business goals are the reason to do data science in the industry and not merely the intellectual joy that comes from looking into the data.
Despite of the comments above, still keep in mind that a stupid question is one which was not asked!&lt;/p&gt;
&lt;h2 id="guiding-questions"&gt;Guiding questions&lt;/h2&gt;
&lt;p&gt;In the paper there are two lists of questions that I believe are very important and helpful.&lt;/p&gt;
&lt;h3 id="for-research-management"&gt;For research management&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;"What is the question we're asking?"&lt;/em&gt; This one is all about alignment; inside the team and with external stakeholders&lt;/li&gt;
&lt;li&gt;&lt;em&gt;"How do we know we have won?"&lt;/em&gt; Definition of acceptance criteria&lt;/li&gt;
&lt;li&gt;&lt;em&gt;"Assuming we solve this problem, what will be build first?"&lt;/em&gt; This one should help us understand whether we're asking the right question&lt;/li&gt;
&lt;li&gt;The last two questions &lt;em&gt;"If everyone in the world uses this, what is the impact?"&lt;/em&gt; and &lt;em&gt;"What's the most evil thing that can be done with this?"&lt;/em&gt; are of more general nature.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="for-organizational-management"&gt;For organizational management&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What are the short-term and long-term goals?&lt;/li&gt;
&lt;li&gt;Who are the supporters and who are the opponents?&lt;/li&gt;
&lt;li&gt;What conflicts are likely to arise?&lt;/li&gt;
&lt;li&gt;What systems are needed to make the data scientists successful?&lt;/li&gt;
&lt;li&gt;What are the costs and time horizons required to implement those systems?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="general-remarks"&gt;General remarks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dashboards are living entities. It is all about finding the balance between empty dashboard on one hand an overwhelmingly full one on the other hand.&lt;/li&gt;
&lt;li&gt;During daily stand ups or other sync meetings adhere to the 3 agile questions:&lt;ul&gt;
&lt;li&gt;What have I done since last sync?&lt;/li&gt;
&lt;li&gt;What shall I do today/till next sync?&lt;/li&gt;
&lt;li&gt;What are the impediments?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Share the teams results with interested stakeholders and try to extend the circle&lt;/li&gt;
&lt;/ul&gt;</content><category term="workflows"></category><category term="research"></category><category term="team"></category></entry><entry><title>Pillars of reproducible data science work</title><link href="http://drorata.github.io/posts/2017/Mar/16/pillars-of-reproducible-data-science-work/" rel="alternate"></link><published>2017-03-16T07:52:53+01:00</published><updated>2017-03-16T08:01:21+01:00</updated><author><name>Dror Atariah</name></author><id>tag:drorata.github.io,2017-03-16:/posts/2017/Mar/16/pillars-of-reproducible-data-science-work/</id><summary type="html">&lt;p&gt;What does it take to have reproducible data science work? Present a minimal example using Docker.&lt;/p&gt;</summary><content type="html">&lt;h1 id="bare-minimum-of-reproducible-work"&gt;Bare minimum of reproducible work&lt;/h1&gt;
&lt;p&gt;The output of software development is naturally a software.
However, when a software developer presents his or her work the compiled binaries are not the whole story.
It is important to present also the source code, which has to be readable, meet the coding conventions of the team, testable, etc. etc.
As software engineering dates back to the 60s of the previous century&lt;sup id="fnref-7d3740b4"&gt;&lt;a class="footnote-ref" href="#fn-7d3740b4"&gt;1&lt;/a&gt;&lt;/sup&gt; the industry has well established methods, paradigms and processes that enable successful software development.
Reproducible work in the context of software development means that the binaries can be recompiled by someone who is not the original developer.&lt;/p&gt;
&lt;p&gt;However, in the context of output of data science work, reproducibility depends on slightly different set of elements:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Result:&lt;/em&gt; It goes without saying that you need to have results if you want to present your work.
One should be able to compare the reproduced result to the original one.
In the case of data science result may be an analysis, report, dashboard, trained and tested model, etc.
Having software development in mind, this part is equivalent to the compiled binaries provided by a developer.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Source code:&lt;/em&gt;  By merely presenting the result of the work, the deliverable is not complete.
Just like in software development, the binaries on their own are not enough to account as a complete work.
It is important to show the way; how the results were achieved.
In other words, sharing the the source code together with the results is crucial.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Data:&lt;/em&gt; Source code alone is not enough.
For data science related work, the source code is not enough as it has to crunch some numbers.
The numbers are available in the data set which was considered when the scientist worked on the task.
Without the data the sources cannot be evaluated and the result cannot be reproduced.
Let me stress it, for real reproduction the very same data set(s) has to be provided alongside the code.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Environment:&lt;/em&gt; Even if you have the sources and the data that the scientist used for generating the results, you might still be lightyears away from being able to reproduce them.
For example, results were generated using the development branch version of Pandas and you only have the stable version installed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In other words, if you want to enable someone to reproduce your work, you have to provide a complete package consisting of all the above items.
The result (or results) must be there, otherwise there's no way of introducing a deliverable.
Next, it might be argued that the simplest item is the source code.
To that end one can harness the rich experience from the world of software development.
But what about the rest?
How do you put everything into one "software" which can be provided for things like&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;peer reviewing&lt;/li&gt;
&lt;li&gt;knowledge sharing&lt;/li&gt;
&lt;li&gt;result reproduction&lt;/li&gt;
&lt;li&gt;versioning control&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you &lt;a href="http://lmgtfy.com/?q=reproducible+research+data+science"&gt;google related keywords&lt;/a&gt; you will find lots of stuff.
I recently read about &lt;a href="http://pachyderm.io/"&gt;Pachyderm&lt;/a&gt;; I didn't have the chance to check it.
You can look in the direction of &lt;a href="https://github.com/airbnb/knowledge-repo"&gt;knowledge-repo&lt;/a&gt;, and probably endless amount of other possibilities.
I decided to start and play with &lt;a href="http://docker.io/"&gt;Docker&lt;/a&gt; when trying to tackle this issue.
True, you might see people  write that this is a &lt;a href="https://blog.wearewizards.io/why-docker-is-not-the-answer-to-reproducible-research-and-why-nix-may-be"&gt;bad solution&lt;/a&gt;, but there are always pros and cons.
You can read more about my first attempt &lt;a href="https://github.com/drorata/mwe-jupyter-docker"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One warning light I see with this approach is the size of the resulting Docker image.
&lt;a href="https://github.com/drorata/mwe-jupyter-docker/blob/230b683ff0d9c2aed4b632185de5af6015bf92c3/Dockerfile"&gt;At first&lt;/a&gt;, I used &lt;code&gt;jupyter/minimal-notebook&lt;/code&gt; as the base of the image.
This resulted in a 3.4GB image.
As per the &lt;a href="https://github.com/jupyter/docker-stacks/issues/205"&gt;related discussion&lt;/a&gt;, I switched to &lt;code&gt;jupyter/base-notebook&lt;/code&gt; and this was a significant improvement.
Still, the size of the resulting image might be a hurdle when it comes to easily share and distribute the results of data science work.
&lt;a href="https://blog.replicated.com/engineering/refactoring-a-dockerfile-for-image-size/"&gt;This post&lt;/a&gt; seems like a good place to look for further size optimization tips.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn-7d3740b4"&gt;
&lt;p&gt;Paul Niquette (1995). &lt;a href="http://www.niquette.com/books/softword/tocsoft.html"&gt;"Softword: Provenance for the Word 'Software'"&lt;/a&gt;. adapted from Sophisticated: The Magazine ISBN 1-58922-233-4&amp;#160;&lt;a class="footnote-backref" href="#fnref-7d3740b4" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="docker"></category><category term="jupyter"></category><category term="reproducible"></category><category term="research"></category></entry><entry><title>Thoughts about data science teams</title><link href="http://drorata.github.io/posts/2017/Feb/17/thoughts-about-data-science-teams/" rel="alternate"></link><published>2017-02-17T15:03:58+01:00</published><updated>2017-02-19T21:59:45+01:00</updated><author><name>Dror Atariah</name></author><id>tag:drorata.github.io,2017-02-17:/posts/2017/Feb/17/thoughts-about-data-science-teams/</id><summary type="html">&lt;p&gt;Some thoughts about the traits needed to establish a successful data science team.&lt;/p&gt;</summary><content type="html">&lt;p&gt;One of the positive things about my current situation (read job hunting), is that it provides a good excuse to meet many different people from many different domains.
This is indeed the case for me.
Furthermore, this state makes you think a lot about your domain, what is your position in it and so on.
You know, the kind of &lt;em&gt;thoughts-about-life&lt;/em&gt; mode.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="thought-about-life" src="http://drorata.github.io/images/life-data-thoughts.jpg"&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h1 id="what-is-a-data-scientist"&gt;What is a data scientist?&lt;/h1&gt;
&lt;p&gt;Probably by now, everyone saw the Venn-diagram by &lt;a href="http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram"&gt;Drew Conway&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="Drew Conway DS Venn-diagram" src="http://static1.squarespace.com/static/5150aec6e4b0e340ec52710a/t/51525c33e4b0b3e0d10f77ab/1364352052403/Data_Science_VD.png"&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Most of its variations (see for example &lt;a href="http://www.kdnuggets.com/2016/10/battle-data-science-venn-diagrams.html"&gt;KDnuggets&lt;/a&gt;) are nothing but semantic changes.
One of them however is really different; it adds the secret sauce.
Check it out:&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="Stephan Kolassa DS Venn-diagram" src="https://i.stack.imgur.com/aiQeT.png"&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Other variations are normally correct, as long as they are not dealing with a business related setting.
In order to make data science relevant to business, you need an extra pillar; communication skills.
A data scientist, as part of a business organization, has to communicate.
He or she has to talk with various stakeholders; understand their problems; explain his point of view; discuss matters with developers etc.
This, by the way, also relates to visualization skills which are mentioned by many.
Visualization is a great and even mandatory support tool when it comes to communication.
In this post I would like to list the central traits which I consider most important when dealing with data science &lt;em&gt;within&lt;/em&gt; a business organization.&lt;/p&gt;
&lt;h1 id="data-science-ingredients"&gt;Data science ingredients&lt;/h1&gt;
&lt;p&gt;Let's dive deeper into what is needed from a data scientist.
But before doing so, allow me a word of warning.
The traits that I will discuss next are amazingly broad and span across insanely amount of fields.
This means that a single data scientist is, almost by definition, a bad one.
I would argue it is (almost) impossible to find an individual who masters all aspects that are needed in order to have a productive data scientist.
This means that the business would ultimately need a &lt;em&gt;team&lt;/em&gt;.
Naturally, not all fields that we shall discuss are needed in all cases.
Each business has to identify what the needed traits of its data science team are and find the individuals that will build such a team.&lt;/p&gt;
&lt;h2 id="theoretical-knowledge"&gt;Theoretical knowledge&lt;/h2&gt;
&lt;p&gt;If you want to build a predictive model, well, you have to know what a predictive model is.
You also have to know how to evaluate it and understand whether it is better than tossing a coin.
If the business case requires deep learning of images, again, you have to know your way.
Still, even if you managed to come up with the best model ever, that manages to find even &lt;a href="https://en.wikipedia.org/wiki/Schr%C3%B6dinger's_cat"&gt;Schrödinger's cat&lt;/a&gt; in an image, it is not at all helpful if the running time is &lt;span class="math"&gt;\(O(n!)\)&lt;/span&gt;.
In order to become a valuable asset of the organization, a data scientist (or the team) should be familiar with the following fields:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mathematics&lt;/li&gt;
&lt;li&gt;Statistics&lt;/li&gt;
&lt;li&gt;Machine/deep learning&lt;/li&gt;
&lt;li&gt;Computer science&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;True, each of the fields I listed is vast on its own.
It might be that deep knowledge about &lt;a href="http://mathworld.wolfram.com/GaloisGroup.html"&gt;Galois Group&lt;/a&gt; won't be the first thing you'd need when tackling a data science problem.
But having a background in these fields can help you a lot.
Furthermore, you can probably argue it is not at all an exhausting list.
And you are probably correct.
Additional fields might be needed depending on (spoiler) business needs.&lt;/p&gt;
&lt;h2 id="tech-stack-know-how"&gt;Tech-stack know-how&lt;/h2&gt;
&lt;p&gt;Data science has something to do with &lt;a href="https://giphy.com/gifs/you-got-it-dude-aVtdz7iNVPI1W"&gt;data&lt;/a&gt;.
As the data is likely not to be stored in the form of handwritten laboratories diaries, knowing the technology can be helpful.
Big data and data science go hand in hand, the underlying tech stack is an important part of the trade.
Moreover, by being on top of the tech-stack one (who masters the theoretical knowledge) can avoid re-inventing the wheel.
Knowing the relevant tech stack is a must when you need to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;access the data&lt;/li&gt;
&lt;li&gt;train scalable models&lt;/li&gt;
&lt;li&gt;master the ETL process&lt;/li&gt;
&lt;li&gt;present the data&lt;/li&gt;
&lt;li&gt;etc. etc. etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is important to note that there are different specializations as well as varying depth that relates to different tech branches and/or tools.
One does not necessarily have to be a core contributor to Spark and still be a productive user of the framework.
Depending on business needs the exact stack has to be defined.
Of course, since advances in the data science ecosystem are very rapid, it is super important to keep the finger on the pulse.
Here it is worthy to warn: staying on top of all the changes in the tech-stack related to data science is a task on its own.&lt;/p&gt;
&lt;h2 id="business-understanding"&gt;Business understanding&lt;/h2&gt;
&lt;p&gt;Some people argue that data science is agnostic to the business domain.
And in some cases they are right; a data point is just a data point.
But this is not entirely correct and a sound understanding of the business can make the difference between a mediocre data science and an excelling one.
Here is an example.
One of the standard measures of a model you trained is its &lt;a href="https://en.wikipedia.org/wiki/Accuracy_and_precision"&gt;accuracy&lt;/a&gt;; what is the ratio between the true positives and negatives with respect to the whole population.
However, if your positive set is very small and valuable in comparison to the negative set, you are likely to be more interested in the &lt;a href="https://en.wikipedia.org/wiki/Precision_and_recall"&gt;recall&lt;/a&gt;.
In other words, you want to minimize the number of false negatives (deeming an event as negative event though it is positive).
Being able to state this observation needs business domain understanding.&lt;/p&gt;
&lt;p&gt;The message is that those in charge of the data science should (and probably must) be in close contact with business people and make sure they work towards shared goals.
This leads to the next item.&lt;/p&gt;
&lt;h2 id="communication-skills"&gt;Communication skills&lt;/h2&gt;
&lt;p&gt;Data scientists, in smaller organizations for sure, are always in between.
They have to juggle between developers, engineers, management, business people, product and virtually all stakeholders.
It comes as no surprise that all these different facets speak different languages.
On top of all these, data scientists have to speak data.
Furthermore, the discussions between data people and other stakeholders should happen during all phases of the work.
It starts at the brainstorming phase, when everyone comes together and discusses a possible project.
It continuous during the planning phase and the development itself.
Lastly, when results are there to be presented, communication between different stakeholders continues.
Constant communication, free of translation issues, is an important feature of productive and tangible work.
Data scientists are sometimes like Marty McFly trying to explain to the world how great Dr. Emmett Brown is or vice versa --- trying to make Doc comprehend what the world is about.&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="back to the future" src="https://c1.staticflickr.com/3/2330/2162754778_9544d707a3.jpg"&gt;&lt;/center&gt;&lt;/p&gt;
&lt;h1 id="so-what"&gt;So what?&lt;/h1&gt;
&lt;p&gt;I believe there are some morals to be taken from the notes above:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data science team:&lt;/strong&gt; Even if you think unicorns (i.e. someone acing in all fields of data science) can be found, you are likely to waste endless amount of time trying to catch them.
It is much more reasonable and beneficial to focus on team building.
The team should cover all the fields needed by the business.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Have well defined objectives:&lt;/strong&gt; Data science is not black magic and it is not going to solve all problems.
Use the communication skills of the data science &lt;em&gt;team&lt;/em&gt; and define well-posed problems.
This way you will enable a focused efforts on the data front.&lt;/li&gt;
&lt;li&gt;I believe the above notes can be helpful both to &lt;em&gt;data scientists&lt;/em&gt; in turning them into more productive and contributing, and at the same time to different &lt;em&gt;stakeholders&lt;/em&gt; who want to utilize the data science capabilities in the company.&lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="team"></category><category term="training"></category><category term="ace"></category></entry></feed>